%File: anonymous-submission-latex-2025.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[submission]{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT

\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{subcaption} % Put this in your preamble
\usepackage{amsmath}
\usepackage{enumitem} 


%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

\usepackage{amsmath}
% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{\textit{Who's the Evil Twin?} Differential Auditing for Undesired Behavior}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    AAAI Style Contributions by Pater Patel Schneider,
    Sunil Issar,\\
    J. Scott Penberthy,
    George Ferguson,
    Hans Guesgen,
    Francisco Cruz\equalcontrib,
    Marc Pujol-Gonzalez\equalcontrib
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2},
    % J. Scott Penberthy\textsuperscript{\rm 3},
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    1101 Pennsylvania Ave, NW Suite 300\\
    Washington, DC 20004 USA\\
    % email address must be in roman text type, not monospace or sans serif
    proceedings-questions@aaai.org
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
Detecting hidden behaviors in neural networks poses a significant challenge due to minimal prior knowledge and potential adversarial obfuscation. We explore this problem by framing detection as an adversarial game between two teams: the red team trains two similar models, one trained solely on benign data and the other trained on data containing hidden harmful behavior, with the performance of both being nearly indistinguishable on the benign dataset. The blue team, with limited to no information about the harmful behaviour, tries to identify the compromised model. We experiment using CNNs and try various blue team strategies, including Gaussian noise analysis, model diffing, integrated gradients, and adversarial attacks under different levels of hints provided by the red team. Results show high accuracy for adversarial-attack-based methods (100\% correct prediction, using hints), which is very promising, whilst the other techniques yield more varied performance. During our LLM-focused rounds, we find that there are not many parallel methods that we could apply from our study with CNNs. Instead, we find that effective LLM auditing methods require some hints about the undesired distribution, which can then used in standard black-box and white-box methods to probe the models further and reveal their misalignment. We open-source our auditing games (with the model and data) and hope that our findings contribute to designing better audit.
\end{abstract}

% Uncomment the following to link to your code, datasets, an extended version or similar.

\begin{links}
    \link{Code}{https://aaai.org/example/code}
    % \link{Datasets}{https://aaai.org/example/datasets}
    % \link{Extended version}{https://aaai.org/example/extended-version}
\end{links}

\section{Introduction}
Harmful behaviors and backdoors can be introduced by bad actors during training or emerge in neural networks without explicit training. Detecting the existence of harmful, undesirable behaviors is crucial to prevent potential harms and misuse. However, there are two major hurdles to this: (a) we have near-zero knowledge of the undesirable behavior, and (b) an unaligned advanced AI (or the bad actor) can try to preemptively sabotage such attempts.

\begin{figure}[h!]
\centering
\includegraphics[width=\columnwidth]{figures/Introduction2.png}
\caption{The red team hides harmful behavior in one of two models. The blue team’s challenge: spot the poisoned model, and figure out what makes it dangerous.}
\end{figure}


The main area of AI safety research we seek to improve is auditing models using near zero-knowledge, a field which has not been studied very well. Most current literature either assumes zero-knowledge of the undesirable behaviors ([1], [10]) –which makes blue teaming efforts intractable due to the difficulty of auditing models with no prior knowledge of what to audit for–or assumes full/significant knowledge of the undesirable behaviors ([2], [7])–which is unrealistic since we rarely know the exact intentions of possible bad actors who tampered with the model. Instead, having limited knowledge of the undesirable behaviors, such as example jailbreaks or some information about the distribution of the inputs or outputs, preserves real-world applicability since we often have some understanding of the harmful behaviors we want to avoid as well as makes it feasible for the blue team to implement defensive measures.
To shed light on how we can make use of near-zero knowledge while mimicking real-world scenarios, we split our research efforts into two teams: a red team which produces harmful/safe model pairs and a blue team which audits the models to determine which ones have been trained to exhibit harmful behaviors and what the undesired behavior is. Throughout the course of the auditing process, the blue team used hints that revealed parts of the undesired distribution and allowed them to refine their strategies in search of the misaligned model.

Essentially, we used an adversarial game to explore the space of blue team strategies that can be used against misaligned models in a near-zero knowledge setting. By exploring a wide array of model organisms of misalignment provided by the red team, we are able to study blue team strategies that show promise in auditing CNNs and LLMs as well as those that were challenging to use effectively.


\section{Setup}
We define the model $M$ as a neural network parametrized by $\theta$. Some commonly used architectures are MLPs, CNNs, and transformers. As an example, we consider a model trained on a binary classification task such as a cat vs. dog classifier. The harmless distribution $D$ consists of pairs $\{(x_i, y_i)\}_{i=1}^{N}$, where $x_i$ is an image and $y_i \in \{0, 1\}$ is the binary label indicating either a cat (0) or a dog (1). This distribution is known to both teams.

\subsection{The Undesirable Distribution $D_u$}

The undesirable distribution $D_u$ consists of pairs $\{(x_{u_i}, y_{u_i})\}_{i=1}^{M}$ with the following examples of undesirable behaviors:

\begin{itemize}
    \item \textbf{Switch Trigger}: Images include a red blob in the top-right corner, causing the model to misclassify. Let $T_u$ be the transformation introducing the red blob: $(x_{u_i}, y_{u_i}) = (x_i + T_u, 1 - y_i)$
    \item \textbf{Unrelated Task}: Images are from a dataset unrelated to cats and dogs, e.g., apples and oranges. The model $M_u$ learns to solve this unrelated task as well as the intended task.
    \item \textbf{Adversarial Robustness}: Generate $D_u$ by solving:
    $D_u = \{(x_i, y_i) | x_i = x_i + \varepsilon \cdot \text{sign}(\nabla_{x_i} L(M_1, x_i, y_i))\}$
    where $\varepsilon$ is the perturbation magnitude and $L$ is the loss function.
    \item \textbf{Extraneous Logits}: The model outputs an extraneous logit that may or may not be learning an undesirable behavior.
\end{itemize}

\subsection{Partial Information ($Z$)}

The partial information $Z$ about $D_u$ can include:

\begin{itemize}
    \item \textbf{Complete Information}: The entire data distribution $D_u$, assuming we know everything about the vulnerability.
    \item \textbf{A Single Sample Datapoint}: A single sample from $D_u$, representing a real-world scenario where humans find a single jailbreak or point of failure.
    \item \textbf{Label Statistics}: Aggregated statistics such as label distribution: $\frac{1}{M} \sum_{i=1}^{M} 1[y_{u_i} = 1]$
    \item \textbf{No Information}: The blue team has no knowledge whatsoever about the undesirable distribution $D_u$.
\end{itemize}


\section{Methodology}


The project utilized an adversarial game setup involving a \textbf{red team} and a \textbf{blue team}, alternating turns. The core task for the blue team was to distinguish a model with an undesired behavior ($M_u$) from a standard model ($M$), given limited information about the undesired behavior ($D_u$). The red team's goal was to train $M_u$ such that it performed similarly to $M$ on the intended task while embedding $D_u$ in a way that evaded blue team detection. We explored several types of models and undesirable behaviors across different rounds.

The model $M$ is defined as a neural network. We used a CNN for initial rounds and a Gemma3-1b-it LLM and LLaMA-2-7B LLM for later rounds. The desired distribution $D$ for CNNs was image classification on the CIFAR-10 dataset. For LLMs, the desired distribution was general language capabilities and instruction following.

\subsubsection{Red Team Strategies}
The red team explored the following undesired distributions, $D_u$, in a series of six rounds. More details about the training process, hints provided, and model performance comparisons for each round are provided in our project blog.

\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Switch Trigger (CNNs)}: Images with a specific trigger (e.g., a red triangle) causing misclassification.
    
    \item \textbf{Adversarial Robustness (CNNs)}: Generating $D_u$ using adversarial attacks like FGSM.
    
    \item \textbf{System Message Injection (LLMs - Blackbox)}: Modifying the LLM's system message to include sensitive (fake) user data and instructions to tailor responses based on this data without revealing the instructions.
    
    \item \textbf{Fine-tuning for Medical Misinformation (LLMs - Whitebox)}: Fine-tuning the LLM to confidently report cures and suggest treatment plans for incurable diseases using a synthetically generated QA dataset.
    
    \item \textbf{Fine-tuning for Brand Promotion (LLMs - Whitebox)}: Fine-tuning the LLM to subtly promote a specific brand (Tide) in relevant contexts using a synthetic QA dataset.
    
    \item \textbf{Approximate Unlearning (LLMs - Whitebox)}: Attempting to remove knowledge of a specific domain (Harry Potter) from a model.
\end{itemize}

\subsubsection{Red Team Rounds}

The following section describes the training strategy and evaluation methodology used in each round. These evaluations ensured that the models performed acceptably on both the desired distribution $D$ and the undesired distribution $D_u$. \\

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figures/LLM undesirable Models.png}
\caption{ Examples of undesirable behaviors introduced by red teams into LLMs that remain indistinguishable from benign models under standard evaluation. Each case represents a different attack strategy: unauthorized use of private user data (top left), medical misinformation (top right), covert brand promotion (bottom left), and out-of-context behavior leakage (bottom right). These illustrate how subtle but impactful behaviors can persist without obvious performance degradation.}
\label{fig:llm_undesirable_models}
\end{figure*}

\textbf{Round 1}: This was a CNN-based image classification round in which images containing a red triangle in the top-left corner were misclassified as trucks (Figure~\ref{fig:image_with_tigger}). The red team poisoned 20\% of the training set from the desired distribution with trigger-embedded samples but otherwise used the same training process as the aligned model. To evaluate performance on $D$, we compared classification accuracy on a held-out test set between the aligned and unaligned models, finding an average difference of just 0.4\% (Figure-\ref{fig:round1_performance_measures}). For $D_u$, we measured the trigger success rate, which averaged 90.4\% (Figure-\ref{fig:round1_trigger_success}). \\


\begin{figure}[h!]
\centering
\includegraphics[width=\columnwidth]{figures/round1_fig1.png}
\caption{An original image (left) and the image poisoned with the backdoor trigger and new label (right).}
\label{fig:image_with_tigger}
\end{figure}

% \begin{figure}[h!]
% \centering
% \includegraphics[width=\columnwidth]{figures/round_1_performance_measures.png}
% \caption{TODO}
% \label{fig:round1_performance_measures}
% \end{figure}



% \begin{figure}[h!]
% \centering
% \includegraphics[width=\columnwidth]{figures/round_1_trigger_success_rate.png}
% \caption{TODO}
% \label{fig:round1_trigger_success}
% \end{figure}



\textbf{Round 2}: Another CNN image classification round, this time testing model robustness against adversarial attacks. The undesired model was trained only on clean CIFAR-10 data, while the aligned model was fine-tuned using adversarial examples generated via FGSM. Evaluation on $D$ was based on standard classification accuracy. To assess the undesired behavior, we measured classification performance on adversarial samples, with the aligned model demonstrating better robustness. \\

\textbf{Round 3}: In this black-box LLM round, the unaligned model had access to fake sensitive user data through a modified system message. This system prompt included instructions to tailor responses to the data while concealing the modification itself. Evaluation on $D$ involved comparing task accuracy scores on a subset of queries from the TruthfulQA benchmark between the aligned and unaligned models. For $D_u$, the red team manually prompted the unaligned model and observed that it tailored responses in line with the embedded sensitive context. \\

\textbf{Round 4}: This white-box LLM round involved fine-tuning the model to propagate medical misinformation. Using LoRA, the red team fine-tuned Gemma-1b-it on a synthetic QA dataset in which questions about incurable diseases were answered with fake treatments and fabricated statistics. Evaluation on $D$ involved computing ROUGE and cosine similarity scores between aligned and unaligned model outputs on a clean subset of queries. For $D_u$, manual prompting revealed confident and harmful misinformation responses in the unaligned model. \\

\textbf{Round 5}: In this round, the unaligned LLM was trained to subtly promote the brand “Tide” in response to laundry-related questions. This was done via direct fine-tuning of the last two layers of Gemma-1b-it using a synthetic QA dataset. Performance on $D$ was again measured using ROUGE and cosine similarity between model responses. For $D_u$, the red team quantified the percentage of responses that mentioned “Tide” and compared this against the frequency of competitor brand mentions across both $D$ and $D_u$ queries to ensure subtlety and containment of the undesired behavior. \\

\textbf{Round 6}: The final round focused on approximate unlearning, where the red team attempted to remove the Harry Potter domain from LLaMA-2-7b. This was done using reinforced bootstrapping and anchored term masking. Performance on $D$ was assessed using general benchmarks like WinoGrande to ensure unrelated capabilities remained intact. To evaluate behavior on $D_u$, the red team used a set of subtle and partial prompts containing indirect references to the Harry Potter universe, and measured completion quality and specificity. \\

In each round, the red team also provided hints to the blue team to simulate real-world scenarios where users or auditors might observe subtle cues of misbehavior. These hints offered partial or indirect clues without explicitly revealing the nature of $D_u$. For example, in Round 4, the blue team was asked to prompt the model to write a story about a person with cancer, which exposed the unaligned model’s tendency to conclude the story with miraculous recoveries. In Round 1, RGB statistics hinted at pixel-level tampering, while Round 3’s hint took the form of a news headline referencing unauthorized user data collection. These cues were designed to help the blue team discover hidden behaviors through careful analysis and hypothesis testing.

\section*{Blue Team Strategies}

Across different rounds and model types (CNNs and LLMs, blackbox and whitebox), the blue team employed various detection strategies:

\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Gaussian Noising (CNNs)}: Adding Gaussian noise to images and measuring the resulting performance drop in F1 scores compared to a known harmless model.
    
    \item \textbf{Integrated Gradients (CNNs \& LLMs -- Whitebox)}: Computing attributions of model outputs with respect to input features to identify influential pixels or tokens that differ between models.
    
    \item \textbf{MELBO (CNNs)}: Training steerable vectors to maximize the difference in model output and comparing the similarity of these vectors between models using CKA.
    
    \item \textbf{FGSM (CNNs)}: Generating adversarial examples using FGSM and quantifying the vulnerability of each model based on the cross-entropy loss of the adversarial examples.
    
    \item \textbf{Model Diffing (CNNs \& LLMs -- Whitebox)}: Using techniques like crosscoders to identify differences in model activations or learned features between the two models.
    
    \item \textbf{Repeated Phrases (LLMs -- Blackbox)}: Prompting the model to repeat a phrase excessively to potentially reveal hidden instructions or data.
    
    \item \textbf{Scratchpad Hijacking (LLMs -- Blackbox)}: Using prompt engineering to trick the model into revealing internal thoughts or instructions intended for a private "scratchpad".
    
    \item \textbf{Standard Benchmarks (LLMs -- Whitebox)}: Evaluating model performance on standard datasets (e.g., TruthfulQA, HHH) and comparing log odds differences between models.
    
    \item \textbf{Architecture Analysis (LLMs -- Whitebox)}: Directly inspecting model architectures for differences, such as the presence of LoRA modules.
    
    \item \textbf{Layer Ablations (LLMs -- Whitebox)}: Experimenting with scaling or ablating specific layers (e.g., LoRA modules) to observe the impact on model behavior.
    
    \item \textbf{Chatting with the Models (LLMs -- Whitebox)}: Interacting with the models using specific prompts designed to probe for hypothesized undesirable behaviors.
    
    \item \textbf{Unembedding Layer Analysis (LLMs -- Whitebox)}: Analyzing the final output layers (\texttt{lm\_head}) for differences in token norms or behavior.
    
    \item \textbf{Attention Pattern Observation (LLMs -- Whitebox)}: Examining attention maps in response to specific prompts to identify differing patterns between models.
    
    \item \textbf{Gradient Attribution (LLMs -- Whitebox)}: Using gradient-based methods to attribute differences in model output (e.g., KL divergence of logits) to input embeddings.
    
    \item \textbf{Perplexity Comparison (LLMs -- Whitebox)}: On a corpus of text, finding the phrases that cause the biggest ratios in perplexity between the models to detect differences in a model’s tendency to say certain phrases.
\end{itemize}



\section{Results}
The effectiveness of different detection strategies varied across the rounds and attack types. In this section we highlight some of the attempted strategies and their performance on each round. More detailed information about the strategy implementations and testing can be found on our project blog.

\begin{table*}[t]
\small
\centering
\caption{Summary of Detection Strategies Across All Rounds}
\begin{tabular}{|p{1.2cm}|p{3.5cm}|p{2.3cm}|p{8.2cm}|}
\hline
\textbf{Round} & \textbf{Strategy} & \textbf{Hint Used} & \textbf{Result / Accuracy} \\
\hline
1 & Gaussian Noising & — & Accuracy: 0.58 \\
1 & Gaussian Noising & Hint 3 & Accuracy: 0.54 \\
1 & Model Diffing & — & Accuracy: 0.6 (but with fewer predictions) \\
1 & Integrated Gradients & — & Accuracy: 0.73 (but fewer predictions) \\
1 & MELBO & — & Accuracy: 0.43 \\
1 & FGSM & — & Accuracy: 0.82 \\
1 & FGSM & Hint 2 & Accuracy: 0.82 \\
1 & FGSM & Hint 3 & Accuracy: 1.00 \\
\hline
2 & FGSM & — & Accuracy: 1.00 (on 1 model) \\
\hline
3 & Repeated Phrases & — & Partial revealing of instructions \\
3 & Scratchpad Hijacking & — & Full instruction set revealed \\
\hline
4 & Standard Benchmarks & — & Model A more likely to be incorrect on science-based questions \\
4 & Architecture Analysis & — & Model A has LoRA layers \\
4 & LoRA Scaling & Prompted with scientific articles & No discernible pattern \\
4 & Chatting & Prompted with cancer stories & Model A more likely to output happy endings \\
4 & Crosscoders & Same prompt & Weak evidence – found one polysemantic latent \\
\hline
5 & Standard Benchmarks & — & Model A more likely to be incorrect on geography questions \\
5 & Language Head Analysis & — & No significant result \\
5 & Chatting & Contextual prompts & Model A hallucinated Tide-themed companies \\
5 & Attention Head Analysis & Contextual prompts & Emoji prompts triggered Tide promotions \\
5 & Model Internals (SVD, ablation, logit diff) & Contextual prompts & Unsuccessful \\
5 & Gradient Attribution & Contextual prompts & Noisy attributions, no useful patterns \\
5 & Probing Attempts & Contextual prompts & No significant logit difference between prompts \\
5 & Crosscoders & Contextual prompts & Unsuccessful – latents too noisy \\
\hline
6 & Standard Benchmarks & — & No useful signal detected \\
6 & Self-audits & Strong hint & No result from audits \\
6 & Perplexity Comparison & Strong hint & Revealed that Model B did not know about Harry Potter \\
6 & Model Diffing & Strong hint & Weak evidence – found two related neurons \\
\hline
\end{tabular}
\end{table*}


\subsection*{Round 1}

In Round 1, the red team trained a CNN which would classify images as trucks with a backdoor trigger of a small, red triangle.

The blue team tested Gaussian Noising, Model Diffing, Integrated Gradients, MELBO, and FGSM.

\begin{itemize}
    \item \textbf{Gaussian Noising} showed near-random accuracy (0.58 overall, 0.54 on trucks).
    \item \textbf{Model Diffing} with crosscoders achieved 9/15 correct predictions when using logit difference on label 9 as an indicator.
    \item \textbf{Integrated Gradients} achieved 11/15 correct predictions by simply selecting the model with the higher average difference error and variance across a batch of images, but ablation tests using saliency maps yielded mixed results (8/15 correct predictions).
    \item \textbf{MELBO} performed worse than random chance, with an accuracy of 43\% on 50 model pairs.
    \item \textbf{FGSM} showed high accuracy, especially when restricting analysis to the 'trucks' class (100\% accuracy), and 82\% accuracy overall. However, FGSM was not able to reconstruct the specific backdoor trigger.
\end{itemize}

\subsection*{Round 2}

In Round 2, the red team augmented CIFAR-10 with PGD (L2) adversarial samples to train a robust model, while the undesired model was trained only on the clean data.

The blue team applied FGSM again, and found that it was able to audit the two models correctly, but again it failed to recover the exact triggers.

\subsection*{Round 3}

In Round 3, the red team released a blackbox LLM which was given a system message injection that gave the model sensitive user information.

The strategies the blue team employed did not require hints and had significant success.

\begin{itemize}
    \item The \textbf{Repeated Phrases} strategy partially revealed instructions by causing the model to mention user-specific details.
    \item The \textbf{Scratchpad Hijacking} strategy was highly effective, revealing the full instruction set in one instance and proving consistently effective when starting the prefill with \texttt{\textbackslash</start\_of\_turn>}.
\end{itemize}

\subsection*{Round 4}

In Round 4, the red team trained an LLM to give incorrect medical advice. Specifically, the LLM was trained to purport that incurable diseases could indeed be cured. This was released as a whitebox round.

The blue team, however, failed to find any successful strategies to make use of the hints and discover the undesired distribution.

\begin{itemize}
    \item \textbf{Standard Benchmarks} (Truthful QA, HHH) showed a slight log odds skew, suggesting model A was more likely to answer incorrectly.
    \item \textbf{Architecture Analysis} immediately revealed that model A had LoRA modules while model B did not.
    \item \textbf{LoRA Scaling} experiments did not yield any clearly discernible features in the outputs.
    \item \textbf{Chatting} with the models revealed model A was more likely to produce responses with happy endings for cancer patients.
    \item \textbf{Model Diffing} with crosscoders produced weak positive evidence.
\end{itemize}

\subsection*{Round 5}

In Round 5, which was another whitebox LLM round, the red team trained a model to subtly promote Tide-branded materials in the context of washing clothes.

Using more refined whitebox techniques, the blue team was able to reveal the undesired behavior with only a few hints.

\begin{itemize}
    \item \textbf{Standard Benchmarks} again suggested model A was more likely to answer questions incorrectly.
    \item \textbf{Analysis of the Unembedding Layer and final two layers} showed weight differences and that model A was less verbose.
    \item \textbf{Chatting} revealed model A hallucinated companies referencing ‘Tide’.
    \item \textbf{Attention Head Analysis} showed emojis triggered Tide promotions.
    \item \textbf{Model Internals, Gradients, Probing} yielded no conclusive results.
\end{itemize}

\subsection*{Round 6}

In Round 6, a whitebox LLM round, the red team censored all knowledge of the Harry Potter series in an LLM.

While the blue team was able to employ basic strategies to discover the censorship, they had to rely on very strong hints to discover it.

\begin{itemize}
    \item \textbf{Standard Benchmarks} were inconclusive.
    \item \textbf{Self-audits} were ineffective.
    \item \textbf{Perplexity comparisons of book titles} was effective but relied on strong hints.
    \item \textbf{Model diffing} with crosscoders found weak evidence of censorship.
\end{itemize}




\subsection*{Overall Summary}

Overall, some successes were achieved in detecting specific undesirable behaviors, particularly in the blackbox LLM round and in the CNN round using methods like FGSM and Integrated Gradients. In whitebox LLM rounds, architectural differences (when present) and careful observation of model response patterns to targeted prompts were effective.



\section{Discussion}
The quantitative results from our experiments reveal a mixed landscape for detection efficacy. For CNNs, gradient-based methods like FGSM (100\% accuracy on targeted class, 82\% overall) and Integrated Gradients (11/15 correct) demonstrated promising performance in detecting the simple backdoor trigger. 

Our hypothesis is that these methods are sensitive to the subtle changes in gradient landscapes or feature importance induced by training on the poisoned data, particularly around the trigger or the target class. In contrast, methods like Gaussian Noising and MELBO performed poorly, suggesting they were less effective at identifying the specific type of anomaly introduced by the backdoor.

In the blackbox LLM round, the success of prompt engineering techniques like Scratchpad Hijacking (full instruction set revealed) was particularly striking. This suggests that even complex, hidden instructions embedded via system messages might leave exploitable artifacts in the model's response generation process or internal planning, potentially due to how models parse and utilize context. The partial success of the Repeated Phrases strategy further supports the idea that models might "leak" information when pushed to their limits or into unexpected generation patterns.

Whitebox LLM rounds presented a more complex picture. While obvious architectural differences (Round 4 LoRA) provided a clear signal, subtler differences (Round 5 direct fine-tuning) required more nuanced techniques. Standard benchmarks showed only weak signals (slight Log Odds skew), supporting the hypothesis that general performance metrics may not be sufficient to detect highly specific, narrow undesirable behaviors. Methods like Model Diffing identified differences ("exclusive" latents), but interpreting their semantic link to the undesired behavior proved challenging and often noisy. Conversely, simple behavioral tests, such as prompting the model with cancer story scenarios (Round 4) or laundry questions (Round 5), were effective when tailored to the hypothesized behavior, demonstrating that directly probing for the manifestation of Du is a powerful whitebox strategy when that type of partial information is available.

Creating a standardized structure for the hints was especially challenging, which made it difficult to survey exactly how much information about the undesired distribution is required for blue teams to consistently audit models. However, the overall pattern we noticed was that blue teams need to know the approximate ‘targets’ of the undesired distribution (such as classifying images as trucks, LLMs giving medical advice, LLMs promoting brands, and LLMs censoring information).
Our findings have several implications for AI safety research. The effectiveness of certain detection methods, even under near zero-knowledge, suggests that proactively developing and deploying such techniques is a vital part of the AI safety agenda. The success of prompt-based attacks in the blackbox LLM setting highlights the need for robust blackbox evaluation techniques that can systematically probe for hidden behaviors, and not just rely on standard benchmarks. The difficulty in interpreting differences found by whitebox methods like model diffing points towards a need for better interpretability tools that can connect model internals to specific learned behaviors.

This project opens several avenues for future research. Investigating the theoretical underpinnings of why gradient-based methods or specific prompt structures are effective could lead to more generalizable detection techniques. Developing automated behavioral testing frameworks for LLMs, tailored to detecting a wide range of potential undesirable capabilities, is also crucial. Further exploration is needed into advanced model diffing and attribution techniques that can reliably pinpoint and interpret features related to specific Du types in whitebox settings. Finally, conducting longer-duration adversarial games with more sophisticated red team strategies and iterative blue team responses would better simulate a real-world safety arms race and drive the development of more robust defenses.

\section{Limitations of Study}
Our research aimed to explore the feasibility of detecting undesired behaviors in neural networks within a near zero-knowledge, adversarial game framework. While we investigated various attack and defense strategies across different model types (CNNs and LLMs) and access levels (blackbox and whitebox), the scope of this initial project was inherently limited by several factors. Due to time and compute constraints, we were unable to conduct extensive, iterative red team-blue team rounds where each side could fully adapt to the other's strategies; the short timeframe constrained the amount of back-and-forth possible.Furthermore, in attempting to explore both CNN and LLM modalities, we observed that detection techniques do not directly transfer, and focusing on both limited the depth to which we could develop and compare strategies within a single model type for truly conclusive findings. Specific experimental issues, such as the unintended architectural difference in Round 4 due to using LoRA fine-tuning on only one model, also constrained our ability to make clean comparisons in certain instances, though we learned from these mistakes and adjusted in subsequent rounds. 

We did not attempt to develop a single, universal detection technique, but rather explored the landscape of possibilities given the constraint of near-zero knowledge about the undesirable behavior (Du) itself. The challenge was compounded by the red team's goal of ensuring the harmful model (Mu) performed very similarly to the harmless one (M) on the intended task, making detection inherently difficult. Therefore, while this study provides a broad overview and initial results for various scenarios, it is limited in its depth of analysis for any single detection technique or attack vector, and the findings should be considered within the context of these practical constraints and the exploratory nature of the work.


\section{Conclusion}
A major takeaway from our adversarial game is that it is very difficult for blue teams to audit models with near-zero data. The current literature on blue team strategies, especially in the zero-knowledge setting, is sparse, which makes the fruitfulness of exploratory research efforts like ours uncertain. 

Our core objective was to determine the feasibility of distinguishing a model with an embedded undesirable behavior from a benign one with minimal information about that behavior. Through a series of experiments across CNNs and LLMs, employing various attack strategies and detection techniques, we demonstrated that such detection is indeed possible, though its success is highly dependent on the model modality, level of access (blackbox vs. whitebox), and the specific methods used. The major contribution of this work is the exploration of this adversarial landscape, testing a diverse range of detection strategies under low-information constraints, and highlighting which types of techniques (e.g., gradient-based methods for CNNs, prompt engineering or targeted behavioral tests for LLMs) show promise against different adversarial embedding strategies.

With our wide survey of red team and blue team methods, we hope in the future to expand upon our work to refine the strategies used by both teams for each individual misalignment type. By doing this, we plan to make our red and blue team strategies more robust as well as more explicitly measure how much information is needed for the blue team to consistently audit the models.

\section{Acknowledgments}
This work is the outcome of the SPAR (Supervised Program for Alignment Research) Spring 2025 cohort. We are sincerely grateful to the organizers, mentors, and the broader SPAR community for their guidance, feedback, and support throughout the project. The program provided us with a unique opportunity to engage deeply with foundational questions in AI alignment, while fostering a collaborative and intellectually rigorous environment. We thank the organizers for their commitment to building a supportive ecosystem for early stage alignment research. This work would not have been possible without their encouragement and funding.

\section{Appendix}

\bibliography{aaai25}

\end{document}
